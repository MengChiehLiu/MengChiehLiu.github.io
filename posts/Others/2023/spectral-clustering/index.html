<!doctypehtml><html lang=zh-TW><meta charset=UTF-8><meta content=width=device-width name=viewport><meta content=#222 name=theme-color><meta content="Hexo 6.3.0" name=generator><link href=/images/apple-touch-icon.png rel=apple-touch-icon sizes=180x180><link href=/images/favicon-32x32.png rel=icon sizes=32x32 type=image/png><link href=/images/favicon-16x16.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/safari-pinned-tab.svg rel=mask-icon><link href=/images/site.webmanifest rel=manifest><link href=/css/main.css rel=stylesheet><link crossorigin=anonymous href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css integrity=sha256-/4UQcSmErDzPCMAiuOiWPVVsNN2s3ZY/NsmXNcj0IFc= rel=stylesheet><link crossorigin=anonymous href=https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css integrity=sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE= rel=stylesheet><script class=next-config data-name=main type=application/json>{"hostname":"mengchiehliu.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.15.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜尋...","empty":"我們無法找到任何有關 ${query} 的搜索結果","hits_time":"${hits} 找到 ${time} 個結果","hits":"找到 ${hits} 個結果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src=/js/config.js></script><meta content="前言  在社群網絡中分析中，有時候會想要找出網絡當中的社群(community detection)，一個最直觀的想法是直接對網絡進行分群，那麼分群結果就是各個community了。 如果我們能夠計算各個節點之間的相似性，那麼我們當然可以直接套用傳統的分群方法，如cosine similiarity，但一來是相似度特徵可能很難取得，二來是這樣分群的話就沒有利用到網絡的結構了，這時我們可以改為使用" name=description><meta content=article property=og:type><meta content="Spectral Clustering - 算法解析與 numpy 程式實作" property=og:title><meta content=https://mengchiehliu.github.io/posts/Others/2023/spectral-clustering/index.html property=og:url><meta content=米蟲的程式小窩 property=og:site_name><meta content="前言  在社群網絡中分析中，有時候會想要找出網絡當中的社群(community detection)，一個最直觀的想法是直接對網絡進行分群，那麼分群結果就是各個community了。 如果我們能夠計算各個節點之間的相似性，那麼我們當然可以直接套用傳統的分群方法，如cosine similiarity，但一來是相似度特徵可能很難取得，二來是這樣分群的話就沒有利用到網絡的結構了，這時我們可以改為使用" property=og:description><meta content=zh_TW property=og:locale><meta content=https://mengchiehliu.github.io/images/2023/04/27/cluster_analysis.jpg property=og:image><meta content=https://mengchiehliu.github.io/images/2023/04/27/ratio_cut.png property=og:image><meta content=https://mengchiehliu.github.io/images/2023/04/27/ratio_cut_2.png property=og:image><meta content=2023-04-26T17:53:24.000Z property=article:published_time><meta content=2024-04-14T12:29:04.190Z property=article:modified_time><meta content="Meng-Chieh Liu" property=article:author><meta content="Social Network Analysis" property=article:tag><meta content="Spectral Clustering" property=article:tag><meta content=summary name=twitter:card><meta content=https://mengchiehliu.github.io/images/2023/04/27/cluster_analysis.jpg name=twitter:image><link href=https://mengchiehliu.github.io/posts/Others/2023/spectral-clustering/ rel=canonical><script class=next-config data-name=page type=application/json>{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-TW","comments":true,"permalink":"https://mengchiehliu.github.io/posts/Others/2023/spectral-clustering/","path":"posts/Others/2023/spectral-clustering/","title":"Spectral Clustering - 算法解析與 numpy 程式實作"}</script><script class=next-config data-name=calendar type=application/json>""</script><title>Spectral Clustering - 算法解析與 numpy 程式實作 | 米蟲的程式小窩</title><noscript><link href=/css/noscript.css rel=stylesheet></noscript><body class=use-motion itemscope itemtype=http://schema.org/WebPage><div class=headband></div><main class=main><div class=column><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切換導航欄 class=toggle role=button><span class=toggle-line></span><span class=toggle-line></span><span class=toggle-line></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <i class=logo-line></i> <p class=site-title>米蟲的程式小窩</p> <i class=logo-line></i> </a><p class=site-subtitle itemprop=description>Viva La Vida</div><div class=site-nav-right><div class="toggle popup-trigger" aria-label=搜尋 role=button><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首頁</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>關於</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>標籤</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分類</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>歸檔</a><li class="menu-item menu-item-apps"><a href=/apps/ rel=section><i class="fa fa-gamepad fa-fw"></i>Apps</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜尋 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container><input autocapitalize=off autocomplete=off class=search-input maxlength=80 placeholder=搜尋... spellcheck=false type=search></div><span class=popup-btn-close role=button> <i class="fa fa-times-circle"></i> </span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></header><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目錄<li class=sidebar-nav-overview>本站概要</ul><div class=sidebar-panel-container><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class=nav><li class="nav-item nav-level-1"><a class=nav-link href=#%E5%89%8D%E8%A8%80><span class=nav-number>1.</span> <span class=nav-text> 前言</span></a><li class="nav-item nav-level-1"><a class=nav-link href=#%E5%A4%A7%E7%B6%B1><span class=nav-number>2.</span> <span class=nav-text> 大綱</span></a><li class="nav-item nav-level-1"><a class=nav-link href=#%E5%85%A7%E6%96%87><span class=nav-number>3.</span> <span class=nav-text> 內文</span></a><ol class=nav-child><li class="nav-item nav-level-2"><a class=nav-link href=#mini-cut-approach><span class=nav-number>3.1.</span> <span class=nav-text> Mini-cut Approach</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#balanced-cut-approach><span class=nav-number>3.2.</span> <span class=nav-text> Balanced-cut Approach</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#matrixs><span class=nav-number>3.2.1.</span> <span class=nav-text> Matrixs</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#calculation><span class=nav-number>3.2.2.</span> <span class=nav-text> Calculation</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#minimize><span class=nav-number>3.2.3.</span> <span class=nav-text> Minimize</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#spectral-clustering><span class=nav-number>3.3.</span> <span class=nav-text> Spectral Clustering</span></a></ol><li class="nav-item nav-level-1"><a class=nav-link href=#%E7%B5%90%E8%AA%9E><span class=nav-number>4.</span> <span class=nav-text> 結語</span></a></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author animated" itemprop=author itemscope itemtype=http://schema.org/Person><img alt="Meng-Chieh Liu" class=site-author-image itemprop=image src=/images/avatar.png><p class=site-author-name itemprop=name>Meng-Chieh Liu<div class=site-description itemprop=description>讀會計但一直寫程式的人</div></div><div class="site-state-wrap animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>30</span> <span class=site-state-item-name>文章</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>5</span> <span class=site-state-item-name>分類</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>25</span> <span class=site-state-item-name>標籤</span></a></div></nav></div><div class="links-of-author animated"><span class=links-of-author-item> <a rel="noopener me" title="GitHub → https://github.com/MengChiehLiu" href=https://github.com/MengChiehLiu target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a rel="noopener me" title="Linkedin → https://www.linkedin.com/in/meng-chieh-liu-615952235/" href=https://www.linkedin.com/in/meng-chieh-liu-615952235/ target=_blank><i class="fa-brands fa-linkedin fa-fw"></i>Linkedin</a> </span><span class=links-of-author-item> <a rel="noopener me" title="E-Mail → mailto:jk211896@gmail.com" href=mailto:jk211896@gmail.com target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span></div></div></div></div><div class="sidebar-inner sidebar-post-related"><div class=animated><div class=links-of-blogroll-title><i class="fa fa-signs-post fa-fw"></i> 相關文章</div><ul class=popular-posts><li class=popular-posts-item><a class=popular-posts-link href=/posts/Others/2024/ascii-spinning-cube/ rel=bookmark> <time class=popular-posts-time>2024-03-23</time> <br> Python 隨便寫 - ASCII Spinning Cube </a><li class=popular-posts-item><a class=popular-posts-link href=/posts/DSA/2024/monotonic-stack/ rel=bookmark> <time class=popular-posts-time>2024-04-13</time> <br> Monotonic Stack </a></ul></div></div></aside></div><div class="main-inner post posts-expand"><div class=post-block><article class=post-content itemscope itemtype=http://schema.org/Article lang=zh-TW><link href=https://mengchiehliu.github.io/posts/Others/2023/spectral-clustering/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=/images/avatar.png itemprop=image> <meta content="Meng-Chieh Liu" itemprop=name> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=米蟲的程式小窩 itemprop=name> <meta content=讀會計但一直寫程式的人 itemprop=description> </span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork> <meta content="Spectral Clustering - 算法解析與 numpy 程式實作 | 米蟲的程式小窩" itemprop=name> <meta itemprop=description> </span><header class=post-header><h1 itemprop="name headline" class=post-title>Spectral Clustering - 算法解析與 numpy 程式實作</h1><div class=post-meta-container><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>發表於</span> <time itemprop="dateCreated datePublished" title="創建時間：2023-04-27 01:53:24" datetime=2023-04-27T01:53:24+08:00>2023-04-27</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-folder"></i> </span> <span class=post-meta-item-text>分類於</span> <span itemprop=about itemscope itemtype=http://schema.org/Thing> <a href=/categories/Others/ itemprop=url rel=index><span itemprop=name>Others</span></a> </span> </span><span class=post-meta-item id=busuanzi_container_page_pv title=閱讀次數> <span class=post-meta-item-icon> <i class="far fa-eye"></i> </span> <span class=post-meta-item-text>閱讀次數：</span> <span id=busuanzi_value_page_pv></span> </span><span class=post-meta-break></span><span class=post-meta-item title=文章字數> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>文章字數：</span> <span>1.9k</span> </span><span class=post-meta-item title=所需閱讀時間> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>所需閱讀時間 ≈</span> <span>8 分鐘</span> </span></div></div></header><div class=post-body itemprop=articleBody><h1 id=前言><a class=markdownIt-Anchor href=#前言></a> 前言</h1><p><img alt="Spectral Clustering" src=/images/2023/04/27/cluster_analysis.jpg><p>在社群網絡中分析中，有時候會想要找出網絡當中的社群(community detection)，一個最直觀的想法是直接對網絡進行分群，那麼分群結果就是各個community了。<br> 如果我們能夠計算各個節點之間的相似性，那麼我們當然可以直接套用傳統的分群方法，如cosine similiarity，但一來是相似度特徵可能很難取得，二來是這樣分群的話就沒有利用到網絡的結構了，這時我們可以改為使用基於圖論的分群方法。<br> 今天這篇文章會帶大家簡單瞭解什麼是cut approach, balanced-cut approach以及其代表方法spectral clustering，並示範不依賴其他套件，僅使用numpy實作spectral clustering。</p><span id=more></span><h1 id=大綱><a class=markdownIt-Anchor href=#大綱></a> 大綱</h1><ul><li><a href=#Mini-cut-Approach>Mini-cut approach</a><li><a href=#Balanced-cut-Approach>Balanced-cut approach</a><li><a href=#Spectral-Clustering>Spectral clustering</a></ul><h1 id=內文><a class=markdownIt-Anchor href=#內文></a> 內文</h1><h2 id=mini-cut-approach><a class=markdownIt-Anchor href=#mini-cut-approach></a> Mini-cut Approach</h2><p>Cut approach，顧名思義是將網絡給切開來形成多個網絡，屬於partitional clustering的一種，每個節點只會屬於一個community，並且community之間也不會overlapping，而Mini-cut Approach則是希望在切割網絡時能最小化切掉的edge數量。<br> 聽起來很直觀，但這樣切會有一個問題，那就是有可能會切出一些非常小的community，導致分群不夠balance。<h2 id=balanced-cut-approach><a class=markdownIt-Anchor href=#balanced-cut-approach></a> Balanced-cut Approach</h2><p>為了解決Mini-cut Approach的缺點，Balanced-cut Approach在切割網絡時除了考量原本的edge，另外除以community的node數做加權調整，切割的目標從最小化edge數轉為最小化ratio cut，公式如下:<br> <img alt="Ratio Cut formula" src=/images/2023/04/27/ratio_cut.png><p>公式解釋: 分成k群(k要自己決定)，每次取一個群<code>Pi</code>出來看，分子部分表示將<code>Pi</code>切開的話會切到的edge數，分母部分表示<code>Pi</code>內部的node數，將k個群的結果加總起來就會是整個網路的ratio cut分數。<h3 id=matrixs><a class=markdownIt-Anchor href=#matrixs></a> Matrixs</h3><p>為了要計算Ratio Cut，我們先要來認識一下三種矩陣:<ol><li>Diagonal Degree Matrix 度數矩陣<br> Degree(度數)指的是一個node有多少個鄰居，將這些Degree轉換為對角矩陣(Diagonal Matrix)就得到Diagonal Degree Matrix。</ol><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br></pre><td class=code><pre><span class=line>D = np.array([</span><br><span class=line>  [<span class=number>2</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>],</span><br><span class=line>  [<span class=number>0</span>, <span class=number>4</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>],</span><br><span class=line>  [<span class=number>0</span>, <span class=number>0</span>, <span class=number>2</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>],</span><br><span class=line>  [<span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>3</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>],</span><br><span class=line>  [<span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>2</span>, <span class=number>0</span>, <span class=number>0</span>],</span><br><span class=line>  [<span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>3</span>, <span class=number>0</span>],</span><br><span class=line>  [<span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>2</span>]])</span><br></pre></table></figure><ol start=2><li>Adjacency Matrix 鄰接矩陣<br> 只要是對Graph有些概念的人應該都對Adjacency Matrix不太陌生，Adjacency Matrix描述了node之間的關係，下面例子假設node之間的關係都是雙向的，所以會是個對稱矩陣。</ol><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br></pre><td class=code><pre><span class=line>A = np.array([</span><br><span class=line>  [<span class=number>0</span>, <span class=number>1</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>1</span>, <span class=number>0</span>, <span class=number>0</span>],</span><br><span class=line>  [<span class=number>1</span>, <span class=number>0</span>, <span class=number>1</span>, <span class=number>1</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>1</span>],</span><br><span class=line>  [<span class=number>0</span>, <span class=number>1</span>, <span class=number>0</span>, <span class=number>1</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>],</span><br><span class=line>  [<span class=number>0</span>, <span class=number>1</span>, <span class=number>1</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>1</span>, <span class=number>0</span>],</span><br><span class=line>  [<span class=number>1</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>1</span>, <span class=number>0</span>],</span><br><span class=line>  [<span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>1</span>, <span class=number>1</span>, <span class=number>0</span>, <span class=number>1</span>],</span><br><span class=line>  [<span class=number>0</span>, <span class=number>1</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>1</span>, <span class=number>0</span>]])</span><br></pre></table></figure><ol start=3><li>Laplacian Matrix 調和矩陣(拉普拉斯矩陣)<br> Laplacian Matrix 的算法就是直接將Diagonal Degree Matrix 減去 Adjacency Matrix，聽起來可能讓人滿頭問號，為什麼要這麼做? 這邊先賣個關子，繼續看下去就知道了。</ol><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br></pre><td class=code><pre><span class=line>L = np.array([</span><br><span class=line>  [ <span class=number>2</span>, -<span class=number>1</span>,  <span class=number>0</span>,  <span class=number>0</span>, -<span class=number>1</span>,  <span class=number>0</span>,  <span class=number>0</span>],</span><br><span class=line>  [-<span class=number>1</span>,  <span class=number>4</span>, -<span class=number>1</span>, -<span class=number>1</span>,  <span class=number>0</span>,  <span class=number>0</span>, -<span class=number>1</span>],</span><br><span class=line>  [ <span class=number>0</span>, -<span class=number>1</span>,  <span class=number>2</span>, -<span class=number>1</span>,  <span class=number>0</span>,  <span class=number>0</span>,  <span class=number>0</span>],</span><br><span class=line>  [ <span class=number>0</span>, -<span class=number>1</span>, -<span class=number>1</span>,  <span class=number>3</span>,  <span class=number>0</span>, -<span class=number>1</span>,  <span class=number>0</span>],</span><br><span class=line>  [-<span class=number>1</span>,  <span class=number>0</span>,  <span class=number>0</span>,  <span class=number>0</span>,  <span class=number>2</span>, -<span class=number>1</span>,  <span class=number>0</span>],</span><br><span class=line>  [ <span class=number>0</span>,  <span class=number>0</span>,  <span class=number>0</span>, -<span class=number>1</span>, -<span class=number>1</span>,  <span class=number>3</span>, -<span class=number>1</span>],</span><br><span class=line>  [ <span class=number>0</span>, -<span class=number>1</span>,  <span class=number>0</span>,  <span class=number>0</span>,  <span class=number>0</span>, -<span class=number>1</span>,  <span class=number>2</span>]])</span><br></pre></table></figure><h3 id=calculation><a class=markdownIt-Anchor href=#calculation></a> Calculation</h3><p>現在先假設我們已經知道分群的結果了，並將結果以0,1的形式表示為一個matrix，令這個結果為X，下面結果是我隨便分的，分3群，<code>X[i][j]==1</code>表示node<code>i</code>屬於cluster<code>j</code>。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br></pre><td class=code><pre><span class=line>X = np.array([</span><br><span class=line>  [<span class=number>1</span>, <span class=number>0</span>, <span class=number>0</span>],</span><br><span class=line>  [<span class=number>1</span>, <span class=number>0</span>, <span class=number>0</span>],</span><br><span class=line>  [<span class=number>0</span>, <span class=number>0</span>, <span class=number>1</span>],</span><br><span class=line>  [<span class=number>0</span>, <span class=number>0</span>, <span class=number>1</span>],</span><br><span class=line>  [<span class=number>0</span>, <span class=number>1</span>, <span class=number>0</span>],</span><br><span class=line>  [<span class=number>0</span>, <span class=number>1</span>, <span class=number>0</span>],</span><br><span class=line>  [<span class=number>0</span>, <span class=number>1</span>, <span class=number>0</span>]])</span><br></pre></table></figure><ol><li>Diagonal Degree Matrix 解釋<br> 我們先將X的轉置矩陣與D做內積(<code>X.T.dot(D)</code>):</ol><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line>array([[<span class=number>2</span>, <span class=number>4</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>],</span><br><span class=line>       [<span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>2</span>, <span class=number>3</span>, <span class=number>2</span>],</span><br><span class=line>       [<span class=number>0</span>, <span class=number>0</span>, <span class=number>2</span>, <span class=number>3</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>]])</span><br></pre></table></figure><p>觀察一下結果會發現，這個矩陣就只是是把Degree填到對應的cluster中而已。<br> 接著我們再把上面結果與X做內積(<code>X.T.dot(D).dot(X)</code>):<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line>array([[<span class=number>6</span>, <span class=number>0</span>, <span class=number>0</span>],</span><br><span class=line>       [<span class=number>0</span>, <span class=number>7</span>, <span class=number>0</span>],</span><br><span class=line>       [<span class=number>0</span>, <span class=number>0</span>, <span class=number>5</span>]])</span><br></pre></table></figure><p>這個結果其實就表示<strong>每個cluster的degree總和</strong>。<ol start=2><li>Adjacency Matrix 解釋<br> 接著我們直接將上面公式的D換成A(<code>X.T.dot(A).dot(X)</code>):</ol><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line>array([[<span class=number>2</span>, <span class=number>2</span>, <span class=number>2</span>],</span><br><span class=line>       [<span class=number>2</span>, <span class=number>4</span>, <span class=number>1</span>],</span><br><span class=line>       [<span class=number>2</span>, <span class=number>1</span>, <span class=number>2</span>]])</span><br></pre></table></figure><p>會發現結果表示在<strong>cluster與cluster間互相連接的edge數</strong>。<ol start=3><li>Laplacian Matrix 解釋<br> 如果我們將兩個結果相減的話，會發現每一行的第i個元素(對角)就是我們想要計算的cut數(分子部分)!!!</ol><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line>array([[ <span class=number>4</span>, -<span class=number>2</span>, -<span class=number>2</span>],  <span class=comment># i=0, 4</span></span><br><span class=line>       [-<span class=number>2</span>,  <span class=number>3</span>, -<span class=number>1</span>],  <span class=comment># i=1, 3</span></span><br><span class=line>       [-<span class=number>2</span>, -<span class=number>1</span>,  <span class=number>3</span>]]) <span class=comment># i=2, 3</span></span><br></pre></table></figure><p>接著我們另外計算每個cluster裡面共有多少個node，我們可以將X的轉置後與自己相乘(<code>X.T.dot(X)</code>):<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line>array([[<span class=number>2</span>, <span class=number>0</span>, <span class=number>0</span>],</span><br><span class=line>       [<span class=number>0</span>, <span class=number>3</span>, <span class=number>0</span>],</span><br><span class=line>       [<span class=number>0</span>, <span class=number>0</span>, <span class=number>2</span>]])</span><br></pre></table></figure><p>最後將對角的結果相除後加起來就是算平均就得到Ratio Cut的分數啦~~~<br> (我這邊直接矩陣相除，大家可以自己算。)<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line>array([[ <span class=number>2.</span> , -inf, -inf],</span><br><span class=line>       [-inf,  <span class=number>1.</span> , -inf],</span><br><span class=line>       [-inf, -inf,  <span class=number>1.5</span>]])</span><br></pre></table></figure><p>根據上面矩陣運算的過程，我們能將公式重新整理為:<br> <img alt="ratio cut formula extended" src=/images/2023/04/27/ratio_cut_2.png><p>在第三行的部分，我們另外對X進行標準化(同除以分母)變成了<code>X prime</code>。<h3 id=minimize><a class=markdownIt-Anchor href=#minimize></a> Minimize</h3><p>回到最一開始的問題，我們的目標是要最小化ratio cut的分數，方法就是透過上面的公式去求解<code>X prime</code>矩陣，難過的是，還記的X矩陣裡最一開始只能填入0跟1嗎，想要在這樣嚴格的限制下求出最佳解是個<strong>NP-hard</strong>的問題。<br> 不過不要灰心，我們還是可以放寬條件，套用近似演算法，其中一種方法就是接下來要介紹的Spectral Clustering~~~<h2 id=spectral-clustering><a class=markdownIt-Anchor href=#spectral-clustering></a> Spectral Clustering</h2><p>Spectral Clustering 基於ratio cut，不過X矩陣中可以填入的值變成介於0~1之間，在放寬條件後，這個最小化問題的解可以被證明是求解Laplacian Matrix的前k小個特徵值的特徵向量。<ol><li>Eigenvalue & Eigenvactors<br> 我們可以用<code>np.linalg.eig</code>來替我們計算特徵值與特徵向量，並使用<code>np.argsort</code>來替我們找出最小的k個特徵向量。</ol><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br></pre><td class=code><pre><span class=line><span class=comment># calculate eigenvalue and eigenvector</span></span><br><span class=line>eig_value, eig_vector = np.linalg.eig(L)</span><br><span class=line></span><br><span class=line><span class=comment># get eigenvector with smallest eigenvalue</span></span><br><span class=line>k = <span class=number>3</span></span><br><span class=line>top_k = np.argsort(eig_value)[:k]</span><br><span class=line>top_k_vector = eig_vector[:, top_k]</span><br></pre></table></figure><p>得到的結果如下:<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br></pre><td class=code><pre><span class=line>array([[ <span class=number>0.37796447</span>, -<span class=number>0.45244521</span>, -<span class=number>0.41243936</span>],</span><br><span class=line>       [ <span class=number>0.37796447</span>,  <span class=number>0.17351892</span>, -<span class=number>0.08541899</span>],</span><br><span class=line>       [ <span class=number>0.37796447</span>,  <span class=number>0.54280925</span>, -<span class=number>0.41243936</span>],</span><br><span class=line>       [ <span class=number>0.37796447</span>,  <span class=number>0.32598548</span>, -<span class=number>0.08541899</span>],</span><br><span class=line>       [ <span class=number>0.37796447</span>, -<span class=number>0.58986844</span>, -<span class=number>0.08541899</span>],</span><br><span class=line>       [ <span class=number>0.37796447</span>, -<span class=number>0.09036404</span>,  <span class=number>0.37705765</span>],</span><br><span class=line>       [ <span class=number>0.37796447</span>,  <span class=number>0.09036404</span>,  <span class=number>0.70407803</span>]])</span><br></pre></table></figure><ol start=2><li>K-means<br> 由於得到的特徵值並無法直接拿來做解釋，因此Spectral Clustering另外使用了K-means來為這些特徵值做分群，因為最小的特徵值為0、特徵向量一定是常數，所以我們可以把它刪去，也就是說，如果我們想要分成k群，那我們至少需要使用k-1個特徵向量。<br> 下面程式提供一個簡單的K-means範例，可以根據自己的需求另外設置停止條件。</ol><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br></pre><td class=code><pre><span class=line><span class=keyword>def</span> <span class="title function_">k_means</span>(<span class=params>X, k, max_iters=<span class=number>10000</span></span>):</span><br><span class=line>    <span class=comment># randomly initialize centroids</span></span><br><span class=line>    centroids = X[np.random.choice(<span class=built_in>len</span>(X), k, replace=<span class=literal>False</span>)]</span><br><span class=line>    <span class=keyword>for</span> _ <span class=keyword>in</span> <span class=built_in>range</span>(max_iters):</span><br><span class=line>       <span class=comment># calculate distances</span></span><br><span class=line>       distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=<span class=number>2</span>)</span><br><span class=line>       <span class=comment># assign labels </span></span><br><span class=line>       labels = np.argmin(distances, axis=<span class=number>1</span>)</span><br><span class=line>       <span class=comment># calculate new centroids</span></span><br><span class=line>       centroids = np.array([np.mean(X[labels == i], axis=<span class=number>0</span>) <span class=keyword>for</span> i <span class=keyword>in</span> <span class=built_in>range</span>(k)])</span><br><span class=line>    <span class=keyword>return</span> labels</span><br><span class=line></span><br><span class=line><span class=comment># remove first vector</span></span><br><span class=line>features = top_k_vector[:, <span class=number>1</span>:]</span><br><span class=line><span class=comment># do k-maens</span></span><br><span class=line>labels = k_means(features, k)</span><br></pre></table></figure><p>這樣就得到分群結果啦~~~(注意: 實際執行數字的順序可能不一樣)<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre><td class=code><pre><span class=line>array([<span class=number>2</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>2</span>, <span class=number>1</span>, <span class=number>1</span>], dtype=int64)</span><br></pre></table></figure><h1 id=結語><a class=markdownIt-Anchor href=#結語></a> 結語</h1><p>其實忽略到複雜的數學證明的話，Spectral Clustering的概念還挺直觀的，文章參考了台大資管系的陳建錦教授所使用的上課講義，經過本人吸收轉化撰寫而成。</div><footer class=post-footer><div class=post-copyright><ul><li class=post-copyright-author><strong>作者： </strong>Meng-Chieh Liu<li class=post-copyright-link><strong>文章連結：</strong> <a title="Spectral Clustering - 算法解析與 numpy 程式實作" href=https://mengchiehliu.github.io/posts/Others/2023/spectral-clustering/>https://mengchiehliu.github.io/posts/Others/2023/spectral-clustering/</a><li class=post-copyright-license><strong>版權聲明： </strong>本網誌所有文章除特別聲明外，均採用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 許可協議。轉載請註明出處！</ul></div><div class=post-tags><a href=/tags/Social-Network-Analysis/ rel=tag><i class="fa fa-tag"></i> Social Network Analysis</a><a href=/tags/Spectral-Clustering/ rel=tag><i class="fa fa-tag"></i> Spectral Clustering</a></div><div class=post-nav><div class=post-nav-item><a title="Hello Hexo！NexT 主題太單調？跟著這些美化步驟為部落格加入個人風格！(5)" href=/posts/Blog/2023/Hello-Hexo-5/ rel=prev> <i class="fa fa-chevron-left"></i> Hello Hexo！NexT 主題太單調？跟著這些美化步驟為部落格加入個人風格！(5) </a></div><div class=post-nav-item><a title="MongoDB概念介紹 + PyMongo基本CRUD操作教學" href=/posts/Back-end/2023/pymongo/ rel=next> MongoDB概念介紹 + PyMongo基本CRUD操作教學 <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><div class="comments utterances-container"></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© <span itemprop=copyrightYear>2024</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Meng-Chieh Liu</span></div><div class=busuanzi-count><span class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=訪客總數> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=總瀏覽次數> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer><div aria-label=回到頂端 class=back-to-top role=button><i class="fa fa-arrow-up fa-lg"></i><span>0%</span></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><script crossorigin=anonymous integrity=sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY= src=https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js></script><script src=/js/comments.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/next-boot.js></script><script crossorigin=anonymous integrity=sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc= src=https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js></script><script src=/js/third-party/search/local-search.js></script><script async src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script class=next-config data-name=enableMath type=application/json>false</script><link crossorigin=anonymous href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/katex.min.css integrity=sha256-gMRN4/6qeELzO1wbFa8qQLU8kfuF2dnAPiUoI0ATjx8= rel=stylesheet><script class=next-config data-name=utterances type=application/json>{"enable":true,"repo":"MengChiehLiu/MengChiehLiu.github.io","issue_term":"pathname","theme":"github-light"}</script><script src=/js/third-party/comments/utterances.js></script><div id=sakana style=position:fixed;right:0;bottom:0px;></div><script async onload=initSakanaWidget({"character":"BMO","enable":true,"enable_mobile":false,"controls":false,"threshold":0.15,"customCharacters":[{"base":"chisato","name":"BMO","image":"/images/BMO.png","i":0.08,"s":0.1,"d":0.96,"r":12}],"size":200,"autoFit":false,"bottom":"0px","stroke":{"color":"#b4b4b4","width":10},"rotate":0}) src=https://cdn.jsdelivr.net/npm/sakana-widget@2.3.0/lib/sakana.min.js></script><script>function log(msg) {
        console.log("[hexo-sakana] " + msg);
      }

      function initSakanaWidget(config) {
        if (
          !config.enable_mobile &&
          window.navigator.userAgent.match(
            /(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i
          )
        ) {
          log(
            '检测为移动端，且配置中未开启在移动端启用组件，hexo-sakana 已禁用'
          );
          return;
        }
        for (character of config.customCharacters) {
          if (!["takina", "chisato"].includes(character.base)) {
            log(`无效基础角色 ${character.base}，取消注册`);
            continue;
          }
          if (character.name === "") {
            log("名称为空，取消注册");
            continue;
          }
          const originCharacter = SakanaWidget.getCharacter(character.base);
          originCharacter.initialState = {
            ...originCharacter.initialState,
            ...character,
          };
          originCharacter.image = !character.image ? originCharacter.image : character.image
          SakanaWidget.registerCharacter(character.name, originCharacter);
          log(`注册自定义角色：${character.name}`)
        }
        new SakanaWidget({
          character: config.character,
          size: config.size,
          autoFit: config.autoFit,
          controls: config.controls,
          stroke: config.stroke,
          threshold: config.threshold,
          rotate: config.rotate,
        }).mount("#sakana");
      }</script>